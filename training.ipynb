{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (3.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from torch) (80.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wogn2\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# 환경 관련 import\n",
    "from env.battle_env import YakemonEnv\n",
    "\n",
    "# 모델 관련 import\n",
    "from p_models.pokemon_info import PokemonInfo\n",
    "from p_models.rank_state import RankManager\n",
    "from p_models.status import StatusManager\n",
    "from p_models.move_info import MoveInfo, MoveEffect, StatChange\n",
    "from p_models.battle_pokemon import BattlePokemon\n",
    "from p_models.ability_info import AbilityInfo\n",
    "\n",
    "# 유틸리티 관련 import\n",
    "from utils.type_relation import calculate_type_effectiveness\n",
    "from utils.battle_logics.battle_sequence import battle_sequence, BattleAction\n",
    "from utils.battle_logics.damage_calculator import calculate_move_damage\n",
    "from utils.battle_logics.rank_effect import calculate_rank_effect\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "# 에이전트 관련 import\n",
    "from agent.dddqn_agent import DDDQNAgent\n",
    "\n",
    "# RL 관련 import\n",
    "from RL.reward_calculator import calculate_reward\n",
    "from RL.get_state_vector import get_state\n",
    "\n",
    "# 데이터 관련 import\n",
    "from p_data.move_data import move_data\n",
    "from p_data.ability_data import ability_data\n",
    "from p_data.mock_pokemon import create_mock_pokemon_list\n",
    "\n",
    "# 컨텍스트 관련 import\n",
    "from context.battle_store import battle_store_instance\n",
    "from context.battle_environment import PublicBattleEnvironment, IndividualBattleEnvironment\n",
    "from context.duration_store import duration_store\n",
    "from context.form_check_wrapper import with_form_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역 변수 초기화\n",
    "battle_store = battle_store_instance\n",
    "duration_store = duration_store\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "HYPERPARAMS = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"gamma\": 0.99,\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_end\": 0.01,\n",
    "    \"epsilon_decay\": 0.995,\n",
    "    \"batch_size\": 64,\n",
    "    \"memory_size\": 10000,\n",
    "    \"target_update\": 10,\n",
    "    \"num_episodes\": 1000,\n",
    "    \"save_interval\": 100,\n",
    "    \"test_episodes\": 100,\n",
    "    \"state_dim\": 140,  # get_state_vector의 출력 차원\n",
    "    \"action_dim\": 8,   # 4개의 기술 + 4개의 교체\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "def train_agent(\n",
    "    env,\n",
    "    agent: DDDQNAgent,\n",
    "    num_episodes: int,\n",
    "    save_path: str = 'models',\n",
    "    agent_name: str = 'ddqn'\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    에이전트 학습 함수\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    losses_history = []\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # 모델 저장 디렉토리 생성\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # 하이퍼파라미터 저장\n",
    "    with open(os.path.join(save_path, f'{agent_name}_hyperparams.json'), 'w') as f:\n",
    "        json.dump(HYPERPARAMS, f, indent=4)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # 매 에피소드마다 새로운 포켓몬 팀 생성\n",
    "        my_team = create_mock_pokemon_list()[:6]  # 6마리 선택\n",
    "        enemy_team = create_mock_pokemon_list()[6:12]  # 다른 6마리 선택\n",
    "        \n",
    "        # 각 포켓몬의 기술과 특성 설정\n",
    "        for pokemon in my_team + enemy_team:\n",
    "            # 기술 설정\n",
    "            moves = []\n",
    "            for i in range(4):\n",
    "                move = MoveInfo(\n",
    "                    name=f'기술{i}',\n",
    "                    power=random.randint(40, 120),\n",
    "                    accuracy=random.randint(70, 100),\n",
    "                    pp=random.randint(5, 20),\n",
    "                    type=random.choice(pokemon['base']['types']),\n",
    "                    category=random.choice(['physical', 'special', 'status']),\n",
    "                    effect=MoveEffect(\n",
    "                        stat_changes=[StatChange('atk', 1)],\n",
    "                        status_effect=random.choice(['burn', 'paralyze', 'poison', None]),\n",
    "                        weather=random.choice(['sunny', 'rainy', 'sandstorm', 'hail', None]),\n",
    "                        field=random.choice(['electric', 'psychic', 'grassy', 'misty', None])\n",
    "                    )\n",
    "                )\n",
    "                moves.append(move)\n",
    "            pokemon['base']['moves'] = moves\n",
    "            \n",
    "            # 특성 설정\n",
    "            ability = AbilityInfo(\n",
    "                id=random.randint(1, 100),\n",
    "                name=random.choice(['엽록소', '맹화', '급류', '심록']),\n",
    "                description='강력한 특성',\n",
    "                appear=['rank_change'],\n",
    "                offensive=['damage_buff'],\n",
    "                defensive=['damage_reduction'],\n",
    "                util=['hp_low_trigger'],\n",
    "                un_touchable=False\n",
    "            )\n",
    "            pokemon['ability'] = ability\n",
    "        \n",
    "        # 배틀 환경 초기화\n",
    "        state = env.reset(my_team=my_team, enemy_team=enemy_team)\n",
    "        \n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            # 현재 상태 벡터 생성\n",
    "            state_dict = get_state(\n",
    "                my_team=my_team,\n",
    "                enemy_team=enemy_team,\n",
    "                active_my=env.battle_store.get_active_index(\"my\"),\n",
    "                active_enemy=env.battle_store.get_active_index(\"enemy\"),\n",
    "                public_env=env.public_env.__dict__,\n",
    "                my_env=env.my_env.__dict__,\n",
    "                enemy_env=env.enemy_env.__dict__,\n",
    "                turn=env.turn,\n",
    "                my_effects=env.duration_store.my_effects,\n",
    "                enemy_effects=env.duration_store.enemy_effects\n",
    "            )\n",
    "            state_vector = list(state_dict.values())\n",
    "            \n",
    "            # 행동 선택\n",
    "            action = agent.select_action(state_vector, env.battle_store, env.duration_store)\n",
    "            \n",
    "            # 행동 실행\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # 다음 상태 벡터 생성\n",
    "            next_state_dict = get_state(\n",
    "                my_team=my_team,\n",
    "                enemy_team=enemy_team,\n",
    "                active_my=env.battle_store.get_active_index(\"my\"),\n",
    "                active_enemy=env.battle_store.get_active_index(\"enemy\"),\n",
    "                public_env=env.public_env.__dict__,\n",
    "                my_env=env.my_env.__dict__,\n",
    "                enemy_env=env.enemy_env.__dict__,\n",
    "                turn=env.turn,\n",
    "                my_effects=env.duration_store.my_effects,\n",
    "                enemy_effects=env.duration_store.enemy_effects\n",
    "            )\n",
    "            next_state_vector = list(next_state_dict.values())\n",
    "            \n",
    "            # 보상 계산\n",
    "            reward = calculate_reward(\n",
    "                my_team=my_team,\n",
    "                enemy_team=enemy_team,\n",
    "                active_my=env.battle_store.get_active_index(\"my\"),\n",
    "                active_enemy=env.battle_store.get_active_index(\"enemy\"),\n",
    "                public_env=env.public_env.__dict__,\n",
    "                my_env=env.my_env.__dict__,\n",
    "                enemy_env=env.enemy_env.__dict__,\n",
    "                turn=env.turn,\n",
    "                my_effects=env.duration_store.my_effects,\n",
    "                enemy_effects=env.duration_store.enemy_effects,\n",
    "                action=action,\n",
    "                done=done,\n",
    "                battle_store=env.battle_store,\n",
    "                duration_store=env.duration_store\n",
    "            )\n",
    "            \n",
    "            # 경험 저장\n",
    "            agent.store_transition(state_vector, action, reward, next_state_vector, done)\n",
    "            \n",
    "            # 학습\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                loss = agent.update()\n",
    "                total_loss += loss\n",
    "            \n",
    "            state_vector = next_state_vector\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 에피소드 결과 저장\n",
    "        avg_reward = total_reward / steps\n",
    "        avg_loss = total_loss / steps if total_loss > 0 else 0\n",
    "        rewards_history.append(avg_reward)\n",
    "        losses_history.append(avg_loss)\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            agent.save(os.path.join(save_path, f'{agent_name}_best.pth'))\n",
    "        \n",
    "        # 주기적으로 모델 저장\n",
    "        if (episode + 1) % HYPERPARAMS[\"save_interval\"] == 0:\n",
    "            agent.save(os.path.join(save_path, f'{agent_name}_episode_{episode+1}.pth'))\n",
    "        \n",
    "        # 학습 진행 상황 출력\n",
    "        print(f'Episode {episode+1}/{num_episodes}')\n",
    "        print(f'Average Reward: {avg_reward:.2f}')\n",
    "        print(f'Average Loss: {avg_loss:.4f}')\n",
    "        print(f'Epsilon: {agent.epsilon:.4f}')\n",
    "        print(f'Steps: {steps}')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    return rewards_history, losses_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 함수\n",
    "def plot_training_results(\n",
    "    rewards_history: list,\n",
    "    losses_history: list,\n",
    "    agent_name: str,\n",
    "    save_path: str = 'results'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    학습 결과 시각화\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # 보상 그래프\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards_history)\n",
    "    plt.title(f'{agent_name} Training Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.savefig(os.path.join(save_path, f'{agent_name}_rewards.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 손실 그래프\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_history)\n",
    "    plt.title(f'{agent_name} Training Losses')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.savefig(os.path.join(save_path, f'{agent_name}_losses.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 함수 정의\n",
    "def test_agent(\n",
    "    env,\n",
    "    agent: DDDQNAgent,\n",
    "    num_episodes: int = 10\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    학습된 에이전트 테스트\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # 테스트용 포켓몬 팀 생성\n",
    "        my_team = create_mock_pokemon_list()[:6]\n",
    "        enemy_team = create_mock_pokemon_list()[6:12]\n",
    "        \n",
    "        # 각 포켓몬의 기술과 특성 설정\n",
    "        for pokemon in my_team + enemy_team:\n",
    "            # 기술 설정\n",
    "            moves = []\n",
    "            for i in range(4):\n",
    "                move = MoveInfo(\n",
    "                    name=f'기술{i}',\n",
    "                    power=random.randint(40, 120),\n",
    "                    accuracy=random.randint(70, 100),\n",
    "                    pp=random.randint(5, 20),\n",
    "                    type=random.choice(pokemon['types']),\n",
    "                    category=random.choice(['physical', 'special', 'status']),\n",
    "                    effect=MoveEffect(\n",
    "                        stat_changes=[StatChange('atk', 1)],\n",
    "                        status_effect=random.choice(['burn', 'paralyze', 'poison', None]),\n",
    "                        weather=random.choice(['sunny', 'rainy', 'sandstorm', 'hail', None]),\n",
    "                        field=random.choice(['electric', 'psychic', 'grassy', 'misty', None])\n",
    "                    )\n",
    "                )\n",
    "                moves.append(move)\n",
    "            pokemon['moves'] = moves\n",
    "            \n",
    "            # 특성 설정\n",
    "            ability = AbilityInfo(\n",
    "                id=random.randint(1, 100),\n",
    "                name=random.choice(['엽록소', '맹화', '급류', '심록']),\n",
    "                description='강력한 특성',\n",
    "                appear=['rank_change'],\n",
    "                offensive=['damage_buff'],\n",
    "                defensive=['damage_reduction'],\n",
    "                util=['hp_low_trigger'],\n",
    "                un_touchable=False\n",
    "            )\n",
    "            pokemon['ability'] = ability\n",
    "        \n",
    "        # 배틀 환경 초기화\n",
    "        state = env.reset(my_team=my_team, enemy_team=enemy_team)\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            # 현재 상태 벡터 생성\n",
    "            state_vector = get_state(\n",
    "                my_team=my_team,\n",
    "                enemy_team=enemy_team,\n",
    "                active_my=env.battle_store.active_my,\n",
    "                active_enemy=env.battle_store.active_enemy,\n",
    "                public_env=env.battle_store.public_env,\n",
    "                my_env=env.battle_store.my_env,\n",
    "                enemy_env=env.battle_store.enemy_env,\n",
    "                turn=env.battle_store.turn,\n",
    "                my_effects=env.duration_store.my_effects,\n",
    "                enemy_effects=env.duration_store.enemy_effects\n",
    "            )\n",
    "            \n",
    "            action = agent.select_action(state_vector, env.battle_store, env.duration_store)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = calculate_reward(state, next_state, action, done, env.battle_store, env.duration_store)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        steps_list.append(steps)\n",
    "        \n",
    "        print(f'Test Episode {episode+1}/{num_episodes}')\n",
    "        print(f'Total Reward: {total_reward:.2f}')\n",
    "        print(f'Steps: {steps}')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    avg_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    avg_steps = np.mean(steps_list)\n",
    "    \n",
    "    print(f'\\nTest Results:')\n",
    "    print(f'Average Reward: {avg_reward:.2f} ± {std_reward:.2f}')\n",
    "    print(f'Average Steps: {avg_steps:.2f}')\n",
    "    \n",
    "    return avg_reward, std_reward, avg_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 상태 벡터 길이: 122\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "StatChange.__init__() missing 1 required positional argument: 'stages'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      9\u001b[39m ddqn_agent = DDDQNAgent(\n\u001b[32m     10\u001b[39m     state_dim=state_dim,\n\u001b[32m     11\u001b[39m     action_dim=action_dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     epsilon_decay=HYPERPARAMS[\u001b[33m\"\u001b[39m\u001b[33mepsilon_decay\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# DDDQN 에이전트 학습\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m ddqn_rewards, ddqn_losses = \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddqn_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHYPERPARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_episodes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mddqn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 학습 결과 시각화\u001b[39;00m\n\u001b[32m     28\u001b[39m plot_training_results(\n\u001b[32m     29\u001b[39m     rewards_history=ddqn_rewards,\n\u001b[32m     30\u001b[39m     losses_history=ddqn_losses,\n\u001b[32m     31\u001b[39m     agent_name=\u001b[33m'\u001b[39m\u001b[33mDDDQN\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     32\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(env, agent, num_episodes, save_path, agent_name)\u001b[39m\n\u001b[32m     31\u001b[39m moves = []\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m4\u001b[39m):\n\u001b[32m     33\u001b[39m     move = MoveInfo(\n\u001b[32m     34\u001b[39m         name=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m기술\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m     35\u001b[39m         power=random.randint(\u001b[32m40\u001b[39m, \u001b[32m120\u001b[39m),\n\u001b[32m     36\u001b[39m         accuracy=random.randint(\u001b[32m70\u001b[39m, \u001b[32m100\u001b[39m),\n\u001b[32m     37\u001b[39m         pp=random.randint(\u001b[32m5\u001b[39m, \u001b[32m20\u001b[39m),\n\u001b[32m     38\u001b[39m         \u001b[38;5;28mtype\u001b[39m=random.choice(pokemon[\u001b[33m'\u001b[39m\u001b[33mbase\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtypes\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m     39\u001b[39m         category=random.choice([\u001b[33m'\u001b[39m\u001b[33mphysical\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mspecial\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m     40\u001b[39m         effect=MoveEffect(\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m             stat_changes=[\u001b[43mStatChange\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43matk\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[32m     42\u001b[39m             status_effect=random.choice([\u001b[33m'\u001b[39m\u001b[33mburn\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mparalyze\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpoison\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]),\n\u001b[32m     43\u001b[39m             weather=random.choice([\u001b[33m'\u001b[39m\u001b[33msunny\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrainy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msandstorm\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhail\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]),\n\u001b[32m     44\u001b[39m             field=random.choice([\u001b[33m'\u001b[39m\u001b[33melectric\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpsychic\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgrassy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmisty\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[32m     45\u001b[39m         )\n\u001b[32m     46\u001b[39m     )\n\u001b[32m     47\u001b[39m     moves.append(move)\n\u001b[32m     48\u001b[39m pokemon[\u001b[33m'\u001b[39m\u001b[33mbase\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmoves\u001b[39m\u001b[33m'\u001b[39m] = moves\n",
      "\u001b[31mTypeError\u001b[39m: StatChange.__init__() missing 1 required positional argument: 'stages'"
     ]
    }
   ],
   "source": [
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 환경 초기화\n",
    "    env = YakemonEnv()  # 실제 게임 환경\n",
    "    state_dim = HYPERPARAMS[\"state_dim\"]\n",
    "    action_dim = HYPERPARAMS[\"action_dim\"]\n",
    "    \n",
    "    # DDDQN 에이전트 생성\n",
    "    ddqn_agent = DDDQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        learning_rate=HYPERPARAMS[\"learning_rate\"],\n",
    "        gamma=HYPERPARAMS[\"gamma\"],\n",
    "        epsilon_start=HYPERPARAMS[\"epsilon_start\"],\n",
    "        epsilon_end=HYPERPARAMS[\"epsilon_end\"],\n",
    "        epsilon_decay=HYPERPARAMS[\"epsilon_decay\"]\n",
    "    )\n",
    "    \n",
    "    # DDDQN 에이전트 학습\n",
    "    ddqn_rewards, ddqn_losses = train_agent(\n",
    "        env=env,\n",
    "        agent=ddqn_agent,\n",
    "        num_episodes=HYPERPARAMS[\"num_episodes\"],\n",
    "        agent_name='ddqn'\n",
    "    )\n",
    "    \n",
    "    # 학습 결과 시각화\n",
    "    plot_training_results(\n",
    "        rewards_history=ddqn_rewards,\n",
    "        losses_history=ddqn_losses,\n",
    "        agent_name='DDDQN'\n",
    "    )\n",
    "    \n",
    "    # 학습된 에이전트 테스트\n",
    "    test_agent(\n",
    "        env=env,\n",
    "        agent=ddqn_agent,\n",
    "        num_episodes=HYPERPARAMS[\"test_episodes\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
